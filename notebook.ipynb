{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "from math import floor\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GridSearchCV, cross_validate, cross_val_score, train_test_split\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import Binarizer, StandardScaler\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.metrics import recall_score\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR = 'DATASET-SNAPSHOT-01'\n",
    "\n",
    "PROJECTS = ['abdera', 'activemq', 'airflow', 'arrow', 'calcite', 'flink', 'geode',\n",
    "            'hadoop', 'hbase', 'hudi', 'jena', 'kafka', 'math', 'maven', 'rat']\n",
    "\n",
    "def load_sample_df(project_name):\n",
    "    '''Load a project from disk into a pandas dataframe'''\n",
    "    cols = ['hash', 'msg', 'n_files', 'loc_added', 'loc_removed', 'issue_id', 'is_bug']\n",
    "    filename = os.path.join(DATASET_DIR, project_name + '_samples.csv')\n",
    "    return pd.read_csv(filename, names=cols)\n",
    "    \n",
    "def print_cv_results(pl, X, y, cv=5):\n",
    "    '''Print the results of a cross-validation.'''\n",
    "    scoring = ['precision_macro', 'recall_macro', 'f1_macro']\n",
    "    scores = cross_validate(pl, X, y, scoring=scoring, cv=cv)\n",
    "    for p, r, f1 in zip(scores['test_precision_macro'], scores['test_recall_macro'], scores['test_f1_macro']):\n",
    "        print('p: {}\\tr: {}\\tf1: {}'.format(p, r, f1))\n",
    "\n",
    "def report_results(y_test, predicted):\n",
    "    '''Print the results of a regular train / test split.'''\n",
    "    print(metrics.classification_report(y_test, predicted))\n",
    "    print('Confusion Matrix:')\n",
    "    print(metrics.confusion_matrix(y_test, predicted))\n",
    "\n",
    "def print_predictions(X_test, y_test, predicted, n=5):\n",
    "    '''Print out examples of TP, TN, FP, and FN for a classifier.'''\n",
    "    tps, fps, tns, fns = list(), list(), list(), list()\n",
    "    for x, y, pred in zip(list(X_test['msg']), y_test, predicted):\n",
    "        if y == True and pred == True:\n",
    "            tps.append(x)\n",
    "        if y == False and pred == True:\n",
    "            fps.append(x)\n",
    "        if y == False and pred == False:\n",
    "            tns.append(x)\n",
    "        if y == True and pred == False:\n",
    "            fns.append(x)\n",
    "    # Optionally shuffle the lists here\n",
    "    print('True Positives (Real Bug):')\n",
    "    for msg in tps[:n]: print(msg.splitlines()[0])\n",
    "    print('\\nTrue Negatives (Real Non-bug):')\n",
    "    for msg in tns[:n]: print(msg.splitlines()[0])\n",
    "    print('\\nFalse Positive (Wrong bug):')\n",
    "    for msg in fps[:n]: print(msg.splitlines()[0])\n",
    "    print('\\nFalse Negative (Wrong Non-bug):')\n",
    "    for msg in fns[:n]: print(msg.splitlines()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StrExtractor(BaseEstimator, TransformerMixin):\n",
    "    'Takes in dataframe and extracts a column into a 1D python list of strings.'\n",
    "\n",
    "    def __init__(self, col_name):\n",
    "        self.col_name = col_name\n",
    "\n",
    "    def transform(self, df, y=None):\n",
    "        return [str(e) for e in df[self.col_name]]\n",
    "\n",
    "    def fit(self, df, y=None): return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IntExtractor(BaseEstimator, TransformerMixin):\n",
    "    'Takes in dataframe and extracts a column into a (n_samples, 1) ndarray.'\n",
    "\n",
    "    def __init__(self, col_name):\n",
    "        self.col_name = col_name\n",
    "\n",
    "    def transform(self, df, y=None):\n",
    "        return np.array([int(e) for e in df[self.col_name]]).reshape(-1, 1)\n",
    "\n",
    "    def fit(self, df, y=None): return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LineLimiter(BaseEstimator, TransformerMixin):\n",
    "    'Takes in an array of strings and truncates lines to max_lines.'\n",
    "\n",
    "    def __init__(self, max_lines=1):\n",
    "        self.max_lines = max_lines\n",
    "\n",
    "    def transform(self, str_arr, y=None):\n",
    "        if self.max_lines == -1: return str_arr\n",
    "        return ['\\n'.join(s.splitlines()[:self.max_lines]) for s in str_arr]\n",
    "\n",
    "    def fit(self, df, y=None): return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTokenizer(BaseEstimator, TransformerMixin):\n",
    "    'Takes in an array of strings and returns an array of arrays of tokens.'\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def transform(self, arr, y=None):\n",
    "        return [gensim.utils.simple_preprocess(s) for s in arr]\n",
    "\n",
    "    def fit(self, df, y=None): return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AvgWord2Vec(BaseEstimator, TransformerMixin):\n",
    "    'dd'\n",
    "\n",
    "    def __init__(self, size=100):\n",
    "        self.size = size\n",
    "        self.model = None\n",
    "        self.vocab = None\n",
    "\n",
    "    def transform(self, arr, y=None):\n",
    "        'Transforms a list of list of tokens to 2D numpy array of (samples, word_vecs)'\n",
    "        # Model not yet traiend\n",
    "        if self.model == None: raise ValueError\n",
    "\n",
    "        word_vecs = np.empty([len(arr), self.size])\n",
    "        for row in range(len(arr)):\n",
    "            tokens = arr[row]\n",
    "            # TODO: Consider 0 vecs\n",
    "            vecs = [self.model.wv[t] for t in tokens if t in self.vocab]\n",
    "            mean_vec = np.mean(np.array(vecs), axis=0)\n",
    "            for i in range(len(mean_vec)):\n",
    "                word_vecs[row][i] = mean_vec[i]\n",
    "            # print('{}: {}'.format(len(tokens), mean_vec))\n",
    "        print(word_vecs.shape)\n",
    "        return word_vecs\n",
    "\n",
    "    def fit(self, arr, y=None):\n",
    "        self.model = gensim.models.Word2Vec(sentences=arr, size=self.size)\n",
    "        self.vocab = list(self.model.wv.vocab)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all sample data\n",
    "dfs = [load_sample_df(name) for name in PROJECTS]\n",
    "\n",
    "# Combine into a single dataframe\n",
    "df = pd.concat(dfs)\n",
    "\n",
    "# Shuffle this dataframe\n",
    "np.random.seed(0)\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# # Split into train and test\n",
    "X = df\n",
    "y = df['is_bug']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1/3, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-1e6a6c0d2aea>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# This is our Bag of Words pipeline\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m bow_pl = Pipeline([\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[1;33m(\u001b[0m\u001b[1;34m'msg'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mStrExtractor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'msg'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;33m(\u001b[0m\u001b[1;34m'line_limit'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLineLimiter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_lines\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;33m(\u001b[0m\u001b[1;34m'vect'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mngram_range\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'english'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Pipeline' is not defined"
     ]
    }
   ],
   "source": [
    "# This is our Bag of Words pipeline\n",
    "bow_pl = Pipeline([\n",
    "    ('msg', StrExtractor(col_name='msg')),\n",
    "    ('line_limit', LineLimiter(max_lines=1)),\n",
    "    ('vect', CountVectorizer(ngram_range=(1, 2), stop_words='english')),\n",
    "    ('tfidf', TfidfTransformer(use_idf=True)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bernoulli Naive Bayes\n",
    "pl = Pipeline([\n",
    "    ('bow', bow_pl),\n",
    "    ('clf', BernoulliNB(alpha=0.5)),\n",
    "])\n",
    "\n",
    "print_cv_results(pl, X, y)\n",
    "pred = pl.fit(X_train, y_train).predict(X_test)\n",
    "print_predictions(X_test, y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-70c3e340061a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# SVM (LinearSVC Classifier) Regression\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m pl = Pipeline([\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[1;33m(\u001b[0m\u001b[1;34m'bow'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbow_pl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;33m(\u001b[0m\u001b[1;34m'clf'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLinearSVC\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m ])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Pipeline' is not defined"
     ]
    }
   ],
   "source": [
    "# SVM (LinearSVC Classifier) Regression\n",
    "pl = Pipeline([\n",
    "    ('bow', bow_pl),\n",
    "    ('clf', LinearSVC())\n",
    "])\n",
    "\n",
    "print_cv_results(pl, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non Textual Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_adds_pl = Pipeline([\n",
    "    ('loc_added_ext', IntExtractor(col_name='loc_added')),\n",
    "    ('loc_added_thr', Binarizer())\n",
    "])\n",
    "\n",
    "has_dels_pl = Pipeline([\n",
    "    ('loc_removed_ext', IntExtractor(col_name='loc_removed')),\n",
    "    ('loc_removed_thr', Binarizer())\n",
    "])\n",
    "\n",
    "nontext_pl = FeatureUnion([\n",
    "    ('bow', bow_pl),\n",
    "    ('n_files', IntExtractor(col_name='n_files')),\n",
    "    ('has_adds', has_adds_pl),\n",
    "    ('has_dels', has_dels_pl)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p: 0.8926518215516154\tr: 0.8946309897171532\tf1: 0.8935448991087573\n",
      "p: 0.8904073574599995\tr: 0.8924550259162589\tf1: 0.8913262170923635\n",
      "p: 0.8925794955865008\tr: 0.8947238209228148\tf1: 0.893536635629868\n",
      "p: 0.8946343512794714\tr: 0.8965311468153789\tf1: 0.8954955638904609\n",
      "p: 0.8914941345938737\tr: 0.8939482797589222\tf1: 0.8925652606938594\n"
     ]
    }
   ],
   "source": [
    "pl = Pipeline([\n",
    "    ('features', nontext_pl),\n",
    "    ('clf', BernoulliNB(alpha=0.05)),\n",
    "])\n",
    "\n",
    "print_cv_results(pl, X, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
